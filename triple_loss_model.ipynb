{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "triple loss_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPIEHUIw6wEG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUCExaY16q3T"
      },
      "source": [
        "**数据集下载**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2kAV9WtKTLY"
      },
      "source": [
        "!wget https://digix-algo-challenge.obs.cn-east-2.myhuaweicloud.com/2020/cv/6rKDTsB6sX8A1O2DA2IAq7TgHPdSPxJF/train_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIAT8DezRBJR"
      },
      "source": [
        "!unzip train_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-s034nywQmS"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmluX4mAf6kT"
      },
      "source": [
        "**文件头加载库**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9dR-rlJaBpz"
      },
      "source": [
        "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "import numpy as np\n",
        "import cv2\n",
        "from keras import backend\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.python.keras.utils import conv_utils\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from keras.layers import Input, Flatten, Dense, concatenate,  Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.python.ops import array_ops. math_ops\n",
        "from tensorflow.python.framework import dtypes\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import datasets, layers, optimizers, models, regularizers\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "from keras.initializers import glorot_uniform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoVF9h2kYMrd"
      },
      "source": [
        "**读取label.txt，使其转换为数据集**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkcP0o89bgpH"
      },
      "source": [
        "*参考CSDN博主「Bubbliiiing」的原创文章*\n",
        "*原文链接：https://blog.csdn.net/weixin_44791964/java/article/details/102779878*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhcyVOoVYWeT"
      },
      "source": [
        "def generate_arrays_from_file(label_file,batch_size):\n",
        "    n = len(label_file)\n",
        "    i = 0\n",
        "    while True:\n",
        "      X_train = []\n",
        "      Y_train = []\n",
        "      for b in range(batch_size):\n",
        "        if i==0:\n",
        "          #内部打乱\n",
        "          np.random.shuffle(label_file)\n",
        "        name = label_file[i].split(',')[0]\n",
        "        # 从文件中读取图像\n",
        "        img = image.load_img(\"train_data/\" + name, target_size=(224,224))\n",
        "        img = image.img_to_array(img)\n",
        "        img = np.expand_dims(img, axis=0)\n",
        "        img = preprocess_input(img)\n",
        "        X_train.append(img)\n",
        "        Y_train.append(label_file[i].split(',')[1])\n",
        "        # 读完一个周期后重新开始\n",
        "        i = (i+1) % n\n",
        "      X_train = np.array(X_train)\n",
        "      X_train = X_train.reshape(-1,224,224,3)\n",
        "      Y_train = [int(i) for i in Y_train]\n",
        "      Y_train = np.array(Y_train)\n",
        "      yield (X_train, Y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCuNRxWVfODZ"
      },
      "source": [
        "**数据集训练集验证集划分**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw34IvhXfNMG"
      },
      "source": [
        "with open(\"train_data/label.txt\",\"r\") as f:\n",
        "  items = f.readlines()\n",
        "#外部打乱\n",
        "np.random.seed(10101)\n",
        "np.random.shuffle(items)\n",
        "#train_test_split\n",
        "num_val = int(len(items)*0.1)\n",
        "num_train = len(items) - num_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EpU8ZcicAuh"
      },
      "source": [
        "**使用ResNet50模型，并在此上进行模型微调**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChhoRj_TnBC3"
      },
      "source": [
        "def _pairwise_distances(embeddings, squared=False):\n",
        "    \"\"\"Compute the 2D matrix of distances between all the embeddings.\n",
        "    Args:\n",
        "        embeddings: tensor of shape (batch_size, embed_dim)\n",
        "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
        "                 If false, output is the pairwise euclidean distance matrix.\n",
        "    Returns:\n",
        "        pairwise_distances: tensor of shape (batch_size, batch_size)\n",
        "    \"\"\"\n",
        "    # Get the dot product between all embeddings\n",
        "    # shape (batch_size, batch_size)\n",
        "    dot_product = tf.matmul(embeddings, tf.transpose(embeddings))\n",
        "\n",
        "    # Get squared L2 norm for each embedding. We can just take the diagonal of `dot_product`.\n",
        "    # This also provides more numerical stability (the diagonal of the result will be exactly 0).\n",
        "    # shape (batch_size,)\n",
        "    square_norm = tf.linalg.tensor_diag_part(dot_product)\n",
        "\n",
        "    # Compute the pairwise distance matrix as we have:\n",
        "    # ||a - b||^2 = ||a||^2  - 2 <a, b> + ||b||^2\n",
        "    # shape (batch_size, batch_size)\n",
        "    distances = tf.expand_dims(square_norm, 1) - 2.0 * dot_product + tf.expand_dims(square_norm, 0)\n",
        "\n",
        "    # Because of computation errors, some distances might be negative so we put everything >= 0.0\n",
        "    distances = tf.maximum(distances, 0.0)\n",
        "\n",
        "    if not squared:\n",
        "        # Because the gradient of sqrt is infinite when distances == 0.0 (ex: on the diagonal)\n",
        "        # we need to add a small epsilon where distances == 0.0\n",
        "        mask = tf.compat.v1.to_float(tf.equal(distances, 0.0))\n",
        "        distances = distances + mask * 1e-16\n",
        "\n",
        "        distances = tf.sqrt(distances)\n",
        "\n",
        "        # Correct the epsilon added: set the distances on the mask to be exactly 0.0\n",
        "        distances = distances * (1.0 - mask)\n",
        "\n",
        "    return distances\n",
        "\n",
        "\n",
        "def _get_anchor_positive_triplet_mask(labels):\n",
        "    \"\"\"Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same label.\n",
        "    Args:\n",
        "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
        "    Returns:\n",
        "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
        "    \"\"\"\n",
        "    # Check that i and j are distinct\n",
        "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
        "    indices_not_equal = tf.math.logical_not(indices_equal)\n",
        "\n",
        "    # Check if labels[i] == labels[j]\n",
        "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
        "    labels_equal = tf.math.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
        "\n",
        "    # Combine the two masks\n",
        "    mask = tf.math.logical_and(indices_not_equal, labels_equal)\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def _get_anchor_negative_triplet_mask(labels):\n",
        "    \"\"\"Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.\n",
        "    Args:\n",
        "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
        "    Returns:\n",
        "        mask: tf.bool `Tensor` with shape [batch_size, batch_size]\n",
        "    \"\"\"\n",
        "    # Check if labels[i] != labels[k]\n",
        "    # Uses broadcasting where the 1st argument has shape (1, batch_size) and the 2nd (batch_size, 1)\n",
        "    labels_equal = tf.math.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
        "\n",
        "    mask = tf.math.logical_not(labels_equal)\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def _get_triplet_mask(labels):\n",
        "    \"\"\"Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.\n",
        "    A triplet (i, j, k) is valid if:\n",
        "        - i, j, k are distinct\n",
        "        - labels[i] == labels[j] and labels[i] != labels[k]\n",
        "    Args:\n",
        "        labels: tf.int32 `Tensor` with shape [batch_size]\n",
        "    \"\"\"\n",
        "    # Check that i, j and k are distinct\n",
        "    indices_equal = tf.cast(tf.eye(tf.shape(labels)[0]), tf.bool)\n",
        "    indices_not_equal = tf.math.logical_not(indices_equal)\n",
        "    i_not_equal_j = tf.expand_dims(indices_not_equal, 2)\n",
        "    i_not_equal_k = tf.expand_dims(indices_not_equal, 1)\n",
        "    j_not_equal_k = tf.expand_dims(indices_not_equal, 0)\n",
        "\n",
        "    distinct_indices = tf.math.logical_and(tf.math.logical_and(i_not_equal_j, i_not_equal_k), j_not_equal_k)\n",
        "\n",
        "\n",
        "    # Check if labels[i] == labels[j] and labels[i] != labels[k]\n",
        "    label_equal = tf.math.equal(tf.expand_dims(labels, 0), tf.expand_dims(labels, 1))\n",
        "    i_equal_j = tf.expand_dims(label_equal, 2)\n",
        "    i_equal_k = tf.expand_dims(label_equal, 1)\n",
        "\n",
        "    valid_labels = tf.math.logical_and(i_equal_j, tf.math.logical_not(i_equal_k))\n",
        "\n",
        "    # Combine the two masks\n",
        "    mask = tf.math.logical_and(distinct_indices, valid_labels)\n",
        "\n",
        "    return mask\n",
        "\n",
        "\n",
        "def batch_all_triplet_loss(labels, embeddings, margin, squared=False):\n",
        "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
        "    We generate all the valid triplets and average the loss over the positive ones.\n",
        "    Args:\n",
        "        labels: labels of the batch, of size (batch_size,)\n",
        "        embeddings: tensor of shape (batch_size, embed_dim)\n",
        "        margin: margin for triplet loss\n",
        "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
        "                 If false, output is the pairwise euclidean distance matrix.\n",
        "    Returns:\n",
        "        triplet_loss: scalar tensor containing the triplet loss\n",
        "    \"\"\"\n",
        "    # Get the pairwise distance matrix\n",
        "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
        "\n",
        "    # shape (batch_size, batch_size, 1)\n",
        "    anchor_positive_dist = tf.expand_dims(pairwise_dist, 2)\n",
        "    assert anchor_positive_dist.shape[2] == 1, \"{}\".format(anchor_positive_dist.shape)\n",
        "    # shape (batch_size, 1, batch_size)\n",
        "    anchor_negative_dist = tf.expand_dims(pairwise_dist, 1)\n",
        "    assert anchor_negative_dist.shape[1] == 1, \"{}\".format(anchor_negative_dist.shape)\n",
        "\n",
        "    # Compute a 3D tensor of size (batch_size, batch_size, batch_size)\n",
        "    # triplet_loss[i, j, k] will contain the triplet loss of anchor=i, positive=j, negative=k\n",
        "    # Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)\n",
        "    # and the 2nd (batch_size, 1, batch_size)\n",
        "    triplet_loss = anchor_positive_dist - anchor_negative_dist + margin\n",
        "\n",
        "    # Put to zero the invalid triplets\n",
        "    # (where label(a) != label(p) or label(n) == label(a) or a == p)\n",
        "    mask = _get_triplet_mask(labels)\n",
        "    mask = tf.compat.v1.to_float(mask)\n",
        "    triplet_loss = tf.multiply(mask, triplet_loss)\n",
        "\n",
        "    # Remove negative losses (i.e. the easy triplets)\n",
        "    triplet_loss = tf.maximum(triplet_loss, 0.0)\n",
        "\n",
        "    # Count number of positive triplets (where triplet_loss > 0)\n",
        "    valid_triplets = tf.compat.v1.to_float(tf.greater(triplet_loss, 1e-16))\n",
        "    num_positive_triplets = tf.reduce_sum(valid_triplets)\n",
        "    num_valid_triplets = tf.reduce_sum(mask)\n",
        "    fraction_positive_triplets = num_positive_triplets / (num_valid_triplets + 1e-16)\n",
        "\n",
        "    # Get final mean triplet loss over the positive valid triplets\n",
        "    triplet_loss = tf.reduce_sum(triplet_loss) / (num_positive_triplets + 1e-16)\n",
        "\n",
        "    return triplet_loss, fraction_positive_triplets\n",
        "\n",
        "\n",
        "def batch_hard_triplet_loss(labels, embeddings, margin=0.1, squared=False):\n",
        "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
        "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
        "    Args:\n",
        "        labels: labels of the batch, of size (batch_size,)\n",
        "        embeddings: tensor of shape (batch_size, embed_dim)\n",
        "        margin: margin for triplet loss\n",
        "        squared: Boolean. If true, output is the pairwise squared euclidean distance matrix.\n",
        "                 If false, output is the pairwise euclidean distance matrix.\n",
        "    Returns:\n",
        "        triplet_loss: scalar tensor containing the triplet loss\n",
        "    \"\"\"\n",
        "    # Get the pairwise distance matrix\n",
        "    pairwise_dist = _pairwise_distances(embeddings, squared=squared)\n",
        "\n",
        "    # For each anchor, get the hardest positive\n",
        "    # First, we need to get a mask for every valid positive (they should have same label)\n",
        "    mask_anchor_positive = _get_anchor_positive_triplet_mask(labels)\n",
        "    mask_anchor_positive = tf.compat.v1.to_float(mask_anchor_positive)\n",
        "\n",
        "    # We put to 0 any element where (a, p) is not valid (valid if a != p and label(a) == label(p))\n",
        "    anchor_positive_dist = tf.multiply(mask_anchor_positive, pairwise_dist)\n",
        "\n",
        "    # shape (batch_size, 1)\n",
        "    hardest_positive_dist = tf.reduce_max(anchor_positive_dist, axis=1, keepdims=True)\n",
        "    tf.summary.scalar(\"hardest_positive_dist\", tf.reduce_mean(hardest_positive_dist))\n",
        "\n",
        "    # For each anchor, get the hardest negative\n",
        "    # First, we need to get a mask for every valid negative (they should have different labels)\n",
        "    mask_anchor_negative = _get_anchor_negative_triplet_mask(labels)\n",
        "    mask_anchor_negative = tf.compat.v1.to_float(mask_anchor_negative)\n",
        "\n",
        "    # We add the maximum value in each row to the invalid negatives (label(a) == label(n))\n",
        "    max_anchor_negative_dist = tf.reduce_max(pairwise_dist, axis=1, keepdims=True)\n",
        "    anchor_negative_dist = pairwise_dist + max_anchor_negative_dist * (1.0 - mask_anchor_negative)\n",
        "\n",
        "    # shape (batch_size,)\n",
        "    hardest_negative_dist = tf.reduce_min(anchor_negative_dist, axis=1, keepdims=True)\n",
        "    tf.summary.scalar(\"hardest_negative_dist\", tf.reduce_mean(hardest_negative_dist))\n",
        "\n",
        "    # Combine biggest d(a, p) and smallest d(a, n) into final triplet loss\n",
        "    triplet_loss = tf.maximum(hardest_positive_dist - hardest_negative_dist + margin, 0.0)\n",
        "\n",
        "    # Get final mean triplet loss\n",
        "    triplet_loss = tf.reduce_mean(triplet_loss)\n",
        "\n",
        "    return triplet_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOiEslXqQOLD"
      },
      "source": [
        "from keras import backend as K\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "def hashencode(x):\n",
        "    return ((K.sign(x - 0.5) + 1) / 2)\n",
        "get_custom_objects().update({'hashencode': Activation(swish)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr2LybjKcAfq"
      },
      "source": [
        "# 首先构建不带分类器的预训练模型\n",
        "base_model = ResNet50(weights='imagenet', include_top=False)\n",
        "# 添加全局平均池化层\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = BatchNormalization()(x)\n",
        "embedding = Activation(\"hashencode\")(x)\n",
        "model = Model(inputs=base_model.input, outputs=embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9j7PnEiwlb0S"
      },
      "source": [
        "model.compile(optimizer= Adam(lr=0.0001),loss=batch_hard_triplet_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6__CpAZLvIsI"
      },
      "source": [
        "batch_size = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSF3XuUY5rGV"
      },
      "source": [
        "filepath = \"{epoch:02d}_BS%d.hdf5\" % batch_size\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, period=25)\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6l7KTPbZgVd9"
      },
      "source": [
        "**训练模型**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNaCOm5Me0rc"
      },
      "source": [
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrF72T-wo_We"
      },
      "source": [
        "for layer in model.layers[:120]:\n",
        "   layer.trainable = False\n",
        "for layer in model.layers[120:]:\n",
        "   layer.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYIoRjGZgTEX"
      },
      "source": [
        "model.fit(generate_arrays_from_file(items[:num_train],batch_size),\n",
        "          steps_per_epoch=max(1, num_train//batch_size),\n",
        "          validation_data=generate_arrays_from_file(items[num_train:],batch_size),\n",
        "          validation_steps=max(1, num_val//batch_size),\n",
        "          epochs=5,\n",
        "          initial_epoch=0,\n",
        "          callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAL10kDndioZ"
      },
      "source": [
        "#model.save('/content/drive/Shared drives/my_new_pan/pure_tripleloss_resnet50_5.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2txMBWKDczwm",
        "outputId": "9618825f-f1b0-48ce-f121-78ba02b0cff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "model.fit(generate_arrays_from_file(items[:num_train],batch_size),\n",
        "          steps_per_epoch=max(1, num_train//batch_size),\n",
        "          validation_data=generate_arrays_from_file(items[num_train:],batch_size),\n",
        "          validation_steps=max(1, num_val//batch_size),\n",
        "          epochs=2,\n",
        "          initial_epoch=0,\n",
        "          callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "967/967 [==============================] - 1076s 1s/step - loss: 0.1257 - val_loss: 6.7220\n",
            "Epoch 2/2\n",
            "967/967 [==============================] - 1073s 1s/step - loss: 0.1001 - val_loss: 0.1022\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff586b559e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbG5TDbY3GPI"
      },
      "source": [
        "#model.load_weights('/content/drive/Shared drives/my_new_pan/10_resnet50_finetuned5.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}