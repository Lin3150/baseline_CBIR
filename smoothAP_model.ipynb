{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "smoothAP_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kblbT7Jvp3a2"
      },
      "source": [
        "!wget https://digix-algo-challenge.obs.cn-east-2.myhuaweicloud.com/2020/cv/6rKDTsB6sX8A1O2DA2IAq7TgHPdSPxJF/train_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4ne2kl8p_pH"
      },
      "source": [
        "!unzip train_data.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCkePjAJqO4Q"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deOfYTt4vKdz"
      },
      "source": [
        "from torchvision import datasets, models, transforms\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import time \n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-O2PNlFrTwE"
      },
      "source": [
        "resnet50 = models.resnet50(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1jeIjB5p_jB"
      },
      "source": [
        "count = 0\n",
        "for param in resnet50.parameters():\n",
        "    count += 1\n",
        "    if count < 100:\n",
        "      param.requires_grad = False\n",
        "    else:\n",
        "      param.requires_grad = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHaF0xOmp_fY"
      },
      "source": [
        "#avgpool_inputs = resnet50.avgpool.in_features\n",
        "resnet50.fc = nn.Sequential(\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW55RLxFhXyb"
      },
      "source": [
        "resnet50.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi2R02VHp_ZS"
      },
      "source": [
        "resnet50 = resnet50.to('cuda:0')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAUnqRBzwRur"
      },
      "source": [
        "def sigmoid(tensor, temp=1.0):\n",
        "    \"\"\" temperature controlled sigmoid\n",
        "    takes as input a torch tensor (tensor) and passes it through a sigmoid, controlled by temperature: temp\n",
        "    \"\"\"\n",
        "    exponent = -tensor / temp\n",
        "    # clamp the input tensor for stability\n",
        "    exponent = torch.clamp(exponent, min=-50, max=50)\n",
        "    y = 1.0 / (1.0 + torch.exp(exponent))\n",
        "    return y\n",
        "\n",
        "\n",
        "def compute_aff(x):\n",
        "    \"\"\"computes the affinity matrix between an input vector and itself\"\"\"\n",
        "    return torch.mm(x, x.t())\n",
        "\n",
        "\n",
        "class SmoothAP(torch.nn.Module):\n",
        "    \"\"\"PyTorch implementation of the Smooth-AP loss.\n",
        "    implementation of the Smooth-AP loss. Takes as input the mini-batch of CNN-produced feature embeddings and returns\n",
        "    the value of the Smooth-AP loss. The mini-batch must be formed of a defined number of classes. Each class must\n",
        "    have the same number of instances represented in the mini-batch and must be ordered sequentially by class.\n",
        "    e.g. the labels for a mini-batch with batch size 9, and 3 represented classes (A,B,C) must look like:\n",
        "        labels = ( A, A, A, B, B, B, C, C, C)\n",
        "    (the order of the classes however does not matter)\n",
        "    For each instance in the mini-batch, the loss computes the Smooth-AP when it is used as the query and the rest of the\n",
        "    mini-batch is used as the retrieval set. The positive set is formed of the other instances in the batch from the\n",
        "    same class. The loss returns the average Smooth-AP across all instances in the mini-batch.\n",
        "    Args:\n",
        "        anneal : float\n",
        "            the temperature of the sigmoid that is used to smooth the ranking function. A low value of the temperature\n",
        "            results in a steep sigmoid, that tightly approximates the heaviside step function in the ranking function.\n",
        "        batch_size : int\n",
        "            the batch size being used during training.\n",
        "        num_id : int\n",
        "            the number of different classes that are represented in the batch.\n",
        "        feat_dims : int\n",
        "            the dimension of the input feature embeddings\n",
        "    Shape:\n",
        "        - Input (preds): (batch_size, feat_dims) (must be a cuda torch float tensor)\n",
        "        - Output: scalar\n",
        "    Examples::\n",
        "        >>> loss = SmoothAP(0.01, 60, 6, 256)\n",
        "        >>> input = torch.randn(60, 256, requires_grad=True).cuda()\n",
        "        >>> output = loss(input)\n",
        "        >>> output.backward()\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, anneal, batch_size, num_id, feat_dims):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        anneal : float\n",
        "            the temperature of the sigmoid that is used to smooth the ranking function\n",
        "        batch_size : int\n",
        "            the batch size being used\n",
        "        num_id : int\n",
        "            the number of different classes that are represented in the batch\n",
        "        feat_dims : int\n",
        "            the dimension of the input feature embeddings\n",
        "        \"\"\"\n",
        "        super(SmoothAP, self).__init__()\n",
        "\n",
        "        assert(batch_size%num_id==0)\n",
        "\n",
        "        self.anneal = anneal\n",
        "        self.batch_size = batch_size\n",
        "        self.num_id = num_id\n",
        "        self.feat_dims = feat_dims\n",
        "\n",
        "    def forward(self, preds):\n",
        "        \"\"\"Forward pass for all input predictions: preds - (batch_size x feat_dims) \"\"\"\n",
        "\n",
        "\n",
        "        # ------ differentiable ranking of all retrieval set ------\n",
        "        # compute the mask which ignores the relevance score of the query to itself\n",
        "        mask = 1.0 - torch.eye(self.batch_size) \n",
        "        mask = mask.unsqueeze(dim=0).repeat(self.batch_size, 1, 1)\n",
        "        # compute the relevance scores via cosine similarity of the CNN-produced embedding vectors\n",
        "        sim_all = compute_aff(preds)\n",
        "        sim_all_repeat = sim_all.unsqueeze(dim=1).repeat(1, self.batch_size, 1)\n",
        "        # compute the difference matrix\n",
        "        sim_diff = sim_all_repeat - sim_all_repeat.permute(0, 2, 1)\n",
        "        # pass through the sigmoid\n",
        "        sim_sg = sigmoid(sim_diff, temp=self.anneal) * mask.cuda()\n",
        "        # compute the rankings\n",
        "        sim_all_rk = torch.sum(sim_sg, dim=-1) + 1\n",
        "\n",
        "        # ------ differentiable ranking of only positive set in retrieval set ------\n",
        "        # compute the mask which only gives non-zero weights to the positive set\n",
        "        xs = preds.view(self.num_id, int(self.batch_size / self.num_id), self.feat_dims)\n",
        "        pos_mask = 1.0 - torch.eye(int(self.batch_size / self.num_id))\n",
        "        pos_mask = pos_mask.unsqueeze(dim=0).unsqueeze(dim=0).repeat(self.num_id, int(self.batch_size / self.num_id), 1, 1)\n",
        "        # compute the relevance scores\n",
        "        sim_pos = torch.bmm(xs, xs.permute(0, 2, 1))\n",
        "        sim_pos_repeat = sim_pos.unsqueeze(dim=2).repeat(1, 1, int(self.batch_size / self.num_id), 1)\n",
        "        # compute the difference matrix\n",
        "        sim_pos_diff = sim_pos_repeat - sim_pos_repeat.permute(0, 1, 3, 2)\n",
        "        # pass through the sigmoid\n",
        "        sim_pos_sg = sigmoid(sim_pos_diff, temp=self.anneal) * pos_mask.cuda()\n",
        "        # compute the rankings of the positive set\n",
        "        sim_pos_rk = torch.sum(sim_pos_sg, dim=-1) + 1\n",
        "\n",
        "        # sum the values of the Smooth-AP for all instances in the mini-batch\n",
        "        ap = torch.zeros(1).cuda()\n",
        "        group = int(self.batch_size / self.num_id)\n",
        "        for ind in range(self.num_id):\n",
        "            pos_divide = torch.sum(sim_pos_rk[ind] / (sim_all_rk[(ind * group):((ind + 1) * group), (ind * group):((ind + 1) * group)]))\n",
        "            ap = ap + ((pos_divide / group) / self.batch_size)\n",
        "\n",
        "        return (1-ap)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHH5zPWFp_Oz"
      },
      "source": [
        "loss_func = SmoothAP(0.01, 120, 12, 2048)\n",
        "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, resnet50.parameters()),lr=0.005)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hju61q-JZGux"
      },
      "source": [
        "import random\n",
        "import os\n",
        "from torch.utils.data import DataLoader, Sampler, Dataset\n",
        "from torchvision import datasets,transforms\n",
        "from keras.preprocessing import image\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zY4yrsBWKhM"
      },
      "source": [
        "class customData(Dataset):\n",
        "    def __init__(self, img_path, dataset = 'train'):\n",
        "      self.img_name = img_path\n",
        "      self.dataset = dataset\n",
        "      self.len = len(img_path)\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.img_name)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "      index = item % self.len\n",
        "      # print(\"i={},index={}\".format(i, index))\n",
        "      image_path = self.img_name[index]\n",
        "      img = Image.open(image_path)\n",
        "      img = transforms.RandomResizedCrop(224)(img)\n",
        "      img = transforms.ToTensor()(img)\n",
        "      if img.shape[0] == 1:\n",
        "        img = img.numpy()\n",
        "        img = np.concatenate((img, img, img))\n",
        "        img = torch.from_numpy(img)               \n",
        "      img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(img)\n",
        "      return img\n",
        "    \n",
        "#####################以上为图像数据读取，返回（img, label）#########################\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_H9uTepzoA0"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4POO2RDVRW_"
      },
      "source": [
        "def train_and_valid(model, loss_function, optimizer, file_path = 'train_data', num_class = 12, num_data = 10, epochs=10):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "    best_epoch = 0\n",
        " \n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        " \n",
        "        model.train()\n",
        " \n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        dirlist = os.listdir('train_data')\n",
        "        dirlist.remove('label.txt')\n",
        "        \n",
        "        for i in tqdm(range(400)):\n",
        "            batch_list = []\n",
        "            num_count = 0\n",
        "            for classlabel in random.sample(dirlist,24):\n",
        "                if num_count <= 12:\n",
        "                  imglist = os.listdir(file_path + '/' + classlabel)\n",
        "                  try:\n",
        "                    sample = random.sample(imglist,10)\n",
        "                    num_count += 1\n",
        "                    for i in sample:\n",
        "                      batch_list.append(file_path + '/' + classlabel + '/' + i)\n",
        "                  except:\n",
        "                    pass\n",
        "\n",
        "            image_datasets = customData(img_path=batch_list) \n",
        "            traindataloaders = torch.utils.data.DataLoader(image_datasets,batch_size=120,shuffle=False) \n",
        "          \n",
        "            for i, input in enumerate(traindataloaders):\n",
        "              inputs = input.cuda()\n",
        "                #因为这里梯度是累加的，所以每次记得清零\n",
        "              optimizer.zero_grad()\n",
        " \n",
        "              outputs = model(inputs)\n",
        "\n",
        "              loss = loss_function(outputs)\n",
        "              print(loss)\n",
        "              loss.backward()\n",
        " \n",
        "              optimizer.step()\n",
        " \n",
        "              train_loss += loss * inputs.size(0)\n",
        "              break\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            for i in range(40):\n",
        "                valid_list = []\n",
        "                for classlabel in random.sample(dirlist,24):\n",
        "                  if num_count <= 12:\n",
        "                    imglist = os.listdir(file_path + '/' + classlabel)\n",
        "                    try:\n",
        "                      sample = random.sample(imglist,10)\n",
        "                      num_count += 1\n",
        "                      for i in sample:\n",
        "                        valid_list.append(file_path + '/' + classlabel + '/' + i)\n",
        "                    except:\n",
        "                      pass\n",
        "                image_datasets = customData(img_path=valid_list) \n",
        "                validdataloaders = torch.utils.data.DataLoader(image_datasets,batch_size=120,\n",
        "                                                               shuffle=False)\n",
        "                for input in validdataloaders:\n",
        "                  inputs = input.cuda()\n",
        "                  outputs = model(inputs)\n",
        " \n",
        "                  loss = loss_function(outputs)\n",
        " \n",
        "                  valid_loss += loss * inputs.size(0)\n",
        "                  break  \n",
        "\n",
        "        avg_train_loss = train_loss/400\n",
        "        \n",
        " \n",
        "        avg_valid_loss = valid_loss/40\n",
        "        \n",
        " \n",
        "        history.append([avg_train_loss, avg_valid_loss])\n",
        " \n",
        "        \n",
        " \n",
        "        epoch_end = time.time()\n",
        " \n",
        "        print(\"Epoch: {:03d}, Training: Loss: {:.4f}, \\n\\t\\tValidation: Loss: {:.4f}, Time: {:.4f}s\".format(\n",
        "            epoch+1, avg_valid_loss, avg_valid_loss, epoch_end-epoch_start\n",
        "        ))\n",
        "        \n",
        " \n",
        "        torch.save(model, '/content/drive/Shared drives/my_new_pan/' + str(epoch+1)+'.pt')\n",
        "    return model, history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yntq2II1V4y2"
      },
      "source": [
        "trained_model, history = train_and_valid(resnet50, loss_func, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}